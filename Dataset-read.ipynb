{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "pulses\n",
      "indicators\n",
      "ip_location\n",
      "Data from pulses:\n",
      "(2966,)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_file = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Example: List all tables in the database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Print all table names\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Example: Read data from a specific table (replace 'your_table_name' with an actual table name)\n",
    "table_name = 'pulses'  # Replace with your actual table name\n",
    "cursor.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "\n",
    "# Fetch all rows from the query result\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Print the data\n",
    "print(f\"Data from {table_name}:\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and the connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the table pulses:\n",
      "Column name: id\n",
      "Column name: name\n",
      "Column name: description\n",
      "Column name: author_name\n",
      "Column name: modified\n",
      "Column name: created\n",
      "Column name: public\n",
      "Column name: adversary\n",
      "Column name: TLP\n",
      "Column name: revision\n",
      "Column name: in_group\n",
      "Column name: is_subscribing\n",
      "Column name: malware_family\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_file = r\"C:\\Users\\I745988\\Downloads\\2023-10-25_cti_data_majd\\2023-10-25_cti_data_majd.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Name of the table you want to check\n",
    "table_name = 'pulses'  # Replace with the actual table name\n",
    "\n",
    "# Execute PRAGMA command to get the columns of the table\n",
    "cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "\n",
    "# Fetch all rows from the query result\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "# Print the column details (column_name is the 2nd element in the tuple)\n",
    "print(f\"Columns in the table {table_name}:\")\n",
    "for column in columns:\n",
    "    print(f\"Column name: {column[1]}\")\n",
    "\n",
    "# Close the cursor and connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the table indicators:\n",
      "Column name: id\n",
      "Column name: pulse_id\n",
      "Column name: indicator\n",
      "Column name: type\n",
      "Column name: created\n",
      "Column name: content\n",
      "Column name: title\n",
      "Column name: description\n",
      "Column name: expiration\n",
      "Column name: is_active\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_file = r\"C:\\Users\\I745988\\Downloads\\2023-10-25_cti_data_majd\\2023-10-25_cti_data_majd.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Name of the table you want to check\n",
    "table_name = 'indicators'  # Replace with the actual table name\n",
    "\n",
    "# Execute PRAGMA command to get the columns of the table\n",
    "cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "\n",
    "# Fetch all rows from the query result\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "# Print the column details (column_name is the 2nd element in the tuple)\n",
    "print(f\"Columns in the table {table_name}:\")\n",
    "for column in columns:\n",
    "    print(f\"Column name: {column[1]}\")\n",
    "\n",
    "# Close the cursor and connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the table ip_location:\n",
      "Column name: ip\n",
      "Column name: cityName\n",
      "Column name: countryName\n",
      "Column name: latitude\n",
      "Column name: longitude\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_file = r\"C:\\Users\\I745988\\Downloads\\2023-10-25_cti_data_majd\\2023-10-25_cti_data_majd.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Name of the table you want to check\n",
    "table_name = 'ip_location'  # Replace with the actual table name\n",
    "\n",
    "# Execute PRAGMA command to get the columns of the table\n",
    "cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "\n",
    "# Fetch all rows from the query result\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "# Print the column details (column_name is the 2nd element in the tuple)\n",
    "print(f\"Columns in the table {table_name}:\")\n",
    "for column in columns:\n",
    "    print(f\"Column name: {column[1]}\")\n",
    "\n",
    "# Close the cursor and connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "future = producer.send('indicators-stream', b'Test message')\n",
    "\n",
    "try:\n",
    "    record_metadata = future.get(timeout=10)  # Wait for response to confirm the message\n",
    "    print(f\"Message sent to topic {record_metadata.topic} partition {record_metadata.partition}\")\n",
    "except KafkaError as e:\n",
    "    print(f\"Error sending message: {e}\")\n",
    "finally:\n",
    "    producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Function to initialize Kafka producer\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "# Function to send data to Kafka\n",
    "def send_to_kafka(producer, topic, key, value):\n",
    "    producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()  # Make sure to flush after sending\n",
    "\n",
    "# Function to process and send indicators in batches\n",
    "def process_and_send_to_kafka(indicators, producer, topic, batch_size=100):\n",
    "    for i in range(0, len(indicators), batch_size):\n",
    "        batch = indicators[i:i + batch_size]\n",
    "        serialized_batch = json.dumps(batch)  # Serialize batch to JSON\n",
    "        print(f\"Sending batch {i // batch_size + 1} to Kafka...\")\n",
    "        send_to_kafka(producer, topic, key=None, value=serialized_batch)\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'  # Confluent Kafka broker address\n",
    "KAFKA_TOPIC = 'indicators-stream'  # Correct topic name where messages should be sent\n",
    "\n",
    "# Create Kafka producer\n",
    "producer = create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "# Example indicator data (for testing)\n",
    "indicators = [{'id': i, 'value': f'Indicator {i}'} for i in range(500)]  # Simulate 500 indicators\n",
    "\n",
    "# Process and send indicators to Kafka\n",
    "CHUNK_SIZE = 500  # Number of indicators per batch\n",
    "limited_indicators = indicators[:CHUNK_SIZE]\n",
    "print(f\"Processing and sending the first {len(limited_indicators)} indicators to Kafka...\")\n",
    "\n",
    "process_and_send_to_kafka(limited_indicators, producer, KAFKA_TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "import json\n",
    "\n",
    "# Kafka Consumer Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Your broker's address\n",
    "    'group.id': 'test-consumer-group',          # Consumer group ID\n",
    "    'auto.offset.reset': 'earliest'             # Consume from the earliest message if no offset exists\n",
    "}\n",
    "\n",
    "# Initialize Kafka Consumer\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Subscribe to the topic\n",
    "consumer.subscribe(['indicators-stream'])\n",
    "\n",
    "try:\n",
    "    print(\"Consuming messages from 'indicators-stream'...\")\n",
    "    while True:\n",
    "        # Poll for new messages\n",
    "        msg = consumer.poll(timeout=1.0)  # Wait for up to 1 second for a message\n",
    "        if msg is None:\n",
    "            continue  # No message received\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"End of partition reached {msg.topic()} {msg.partition()} offset {msg.offset()}\")\n",
    "            else:\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # Decode and parse the message\n",
    "            raw_message = msg.value().decode('utf-8')\n",
    "            print(f\"Raw Message: {raw_message}\")\n",
    "            \n",
    "            try:\n",
    "                # Attempt to parse the message as JSON\n",
    "                parsed_message = json.loads(raw_message)\n",
    "                print(\"Parsed Message (JSON):\")\n",
    "                print(json.dumps(parsed_message, indent=4))  # Pretty print JSON\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Message is not in JSON format.\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nConsumption interrupted by user.\")\n",
    "finally:\n",
    "    print(\"Closing consumer...\")\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching first 10 rows from table 'pulses'...\n",
      "Columns in 'pulses': ['id', 'name', 'description', 'author_name', 'modified', 'created', 'public', 'adversary', 'TLP', 'revision', 'in_group', 'is_subscribing', 'malware_family']\n",
      "{'id': '645c9a033dd0e4e34e6cba8f', 'name': 'Malware Bazaar 7', 'description': '', 'author_name': 'LoveAndren', 'modified': '2023-05-11T07:32:19.907000', 'created': '2023-05-11T07:32:19.907000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63ac057b2644355f3788973e', 'name': 'Cyble - New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': 'A look at some of the latest ransomware strains created based on the leaked source code of Conti ransomware, which was discovered in 2021 and is being investigated by Cyble Research and Intelligence Labs (CRIL).', 'author_name': 'CyberHunter_NL', 'modified': '2022-12-28T08:59:38.883000', 'created': '2022-12-28T08:59:38.883000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63abba430849f35bb074ab1a', 'name': 'Cyble: New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': '', 'author_name': 'goatluxy', 'modified': '2022-12-28T03:38:43.688000', 'created': '2022-12-28T03:38:43.688000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a55b8feb56dde69cb1e884', 'name': 'New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': 'New ransomware strains are emerging based on the leaked source code of Conti ransomware, Cyble Research and Intelligence Labs (CRIL) has discovered, along with a new strain named Putin Team and BlueSky Meow.', 'author_name': 'AlienVault', 'modified': '2022-12-23T07:41:02.221000', 'created': '2022-12-23T07:41:02.221000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a54a24d28f6f73643c7aa0', 'name': 'Cyble &mdash; New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': '', 'author_name': 'Tr1sa111', 'modified': '2022-12-23T06:26:44.410000', 'created': '2022-12-23T06:26:44.410000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a549d1845511f75f35d875', 'name': 'Cyble &mdash; New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': '', 'author_name': 'tr2222200', 'modified': '2022-12-23T06:25:21.262000', 'created': '2022-12-23T06:25:21.262000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a549c76a08a43303915377', 'name': 'Cyble &mdash; New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': 'New ransomware strains are emerging based on the leaked source code of Conti ransomware, Cyble Research and Intelligence Labs (CRIL) has discovered, along with a new strain named Putin Team and BlueSky Meow.', 'author_name': 'tr2222200', 'modified': '2022-12-23T06:25:11.417000', 'created': '2022-12-23T06:25:11.417000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a53c463b8a78fd4aafb952', 'name': 'Cyble &mdash; New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': '', 'author_name': 'cyberasmi', 'modified': '2022-12-23T05:27:34.886000', 'created': '2022-12-23T05:27:34.886000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a4bf71b46e295f7c5c664e', 'name': 'New Ransomware Strains Emerging From Leaked Conti’s Source Code ', 'description': '', 'author_name': 'feisty-swim1410', 'modified': '2022-12-22T20:34:57.730000', 'created': '2022-12-22T20:34:57.730000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a481d1b821012188699107', 'name': 'New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': 'New ransomware strains are emerging based on the leaked source code of Conti ransomware, Cyble Research and Intelligence Labs (CRIL) has discovered, along with a new strain named Putin Team and BlueSky Meow.', 'author_name': 'AlienVault', 'modified': '2022-12-22T16:12:00.733000', 'created': '2022-12-22T16:12:00.733000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "\n",
      "Producing rows from 'pulses' to Kafka...\n",
      "\n",
      "Fetching first 10 rows from table 'indicators'...\n",
      "Columns in 'indicators': ['id', 'pulse_id', 'indicator', 'type', 'created', 'content', 'title', 'description', 'expiration', 'is_active']\n",
      "{'id': 3678952388, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '32788cd0818804797beb15135695e165', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 62958d20e3f6b262d16e5d5478c2c0d882ecec72b1e7d26276c07d7187dc0a1e', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952389, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '32dd32a4496e689ba904e92385d350d5', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 82521006cdb75d3826edb12e2d7a038a37cdafddf220cac4ae927596491c9421', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952390, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '3343fe2b2f76cda4c776eb592a4d0146', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Win32/Flooder.Agent', 'description': 'MD5 of 3c4c8fdda37010277359be7d34620f97ac7485270c73568471603913097ffb96', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952391, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '48e074289337a25992d16166bab58c73', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 40b3e23941cb1a50f8061c5feee2ce505833f2b8269ec402018359bf1a399b20', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952392, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '623b083c79bd9855db70cb1509861cb0', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 3ed019075072f4b6744c45753a17e5210bf33405a25c0c285b9e302d341f5210', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952393, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '6654235619b25fd70f0233aec854e0ea', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Arechclient2', 'description': 'MD5 of 83956ff42ffb6c352e3257a55ab76416b120a1cbf6b1f92672d50c4b8fae404f', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952394, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '6cbe61e619d1c2d892e95c7dac37582f', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 3cea6c13531dbf5b2fdd0a3aca1c09bf8efa53d37d6116e13791cac1296540c5', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952395, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '74b7fa40d5864c398a9210c6ef7fed06', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 4a29128a2d1f0bce07b0a16c41d3ceed6354dffe4d22fa526238cd7b74233433', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952396, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '7d805076b1ccffc8a34ca42506dd9a57', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Arechclient2', 'description': 'MD5 of 875972c6043bdc3ac7cc51ac8f7ae58a43b43fcfc933672caa9a0f92f27825c0', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952397, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '969a55f86f1afbbdf9e823be4325ffff', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 45a2970a08952954afcbbb120619f5c0736643a691d26d2563604139fbab180f', 'expiration': None, 'is_active': 1}\n",
      "\n",
      "Producing rows from 'indicators' to Kafka...\n",
      "\n",
      "Fetching first 10 rows from table 'ip_location'...\n",
      "Columns in 'ip_location': ['ip', 'cityName', 'countryName', 'latitude', 'longitude']\n",
      "{'ip': '148.251.234.93', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.162.83', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.189.114', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.208.18', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.40.177', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.104.67', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.110.152', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.147.72', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.184.189', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.20.250', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "\n",
      "Producing rows from 'ip_location' to Kafka...\n",
      "\n",
      "Data ingestion and production completed.\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# SQLite Configuration\n",
    "DB_FILE = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "TABLES = ['pulses', 'indicators', 'ip_location']  # List of table names\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "KAFKA_TOPIC = 'data-stream'\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "# Send data to Kafka\n",
    "def send_to_kafka(producer, topic, key, value):\n",
    "    producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()\n",
    "\n",
    "# Display and ingest data from SQLite\n",
    "def display_and_ingest_data(db_file, tables, producer, kafka_topic, row_limit=10):\n",
    "    # Connect to SQLite\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Loop through each table\n",
    "    for table_name in tables:\n",
    "        print(f\"\\nFetching first {row_limit} rows from table '{table_name}'...\")\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT {row_limit}\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Get column names for better display and JSON serialization\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        \n",
    "        # Display the rows\n",
    "        print(f\"Columns in '{table_name}': {column_names}\")\n",
    "        for row in rows:\n",
    "            print(dict(zip(column_names, row)))\n",
    "\n",
    "        # Serialize and produce each row to Kafka\n",
    "        print(f\"\\nProducing rows from '{table_name}' to Kafka...\")\n",
    "        for row in rows:\n",
    "            serialized_row = json.dumps(dict(zip(column_names, row)))  # Convert to JSON\n",
    "            send_to_kafka(producer, kafka_topic, key=None, value=serialized_row)\n",
    "\n",
    "    print(\"\\nData ingestion and production completed.\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Main Producer Logic\n",
    "producer = create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n",
    "display_and_ingest_data(DB_FILE, TABLES, producer, KAFKA_TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Joined Data Columns:\n",
      "['indicator_id', 'pulse_id', 'indicator', 'type', 'created', 'title', 'description', 'cityName', 'countryName', 'latitude', 'longitude']\n",
      "\n",
      "Joined Data Rows:\n",
      "{'indicator_id': 2233484595, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.234.93', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3762239076, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.162.83', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3748688316, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.189.114', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3462478129, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.208.18', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3628506579, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.40.177', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3747596924, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.104.67', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 2239778756, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.110.152', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3761839080, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.147.72', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3761839089, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.184.189', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3663336263, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.20.250', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# SQLite Configuration\n",
    "DB_FILE = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "ROW_LIMIT = 10  # Limit the number of rows fetched\n",
    "\n",
    "# Perform the join operation between 'indicators' and 'ip_location'\n",
    "def fetch_joined_data(db_file, row_limit):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    join_query = \"\"\"\n",
    "    SELECT \n",
    "        ind.id AS indicator_id,\n",
    "        ind.pulse_id,\n",
    "        ind.indicator,\n",
    "        ind.type,\n",
    "        ind.created,\n",
    "        ind.title,\n",
    "        ind.description,\n",
    "        loc.cityName,\n",
    "        loc.countryName,\n",
    "        loc.latitude,\n",
    "        loc.longitude\n",
    "    FROM \n",
    "        indicators AS ind\n",
    "    INNER JOIN \n",
    "        ip_location AS loc\n",
    "    ON \n",
    "        ind.indicator = loc.ip\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    cursor.execute(join_query, (row_limit,))\n",
    "    joined_rows = cursor.fetchall()\n",
    "\n",
    "    # Get column names for joined data\n",
    "    joined_columns = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Convert rows to dictionary for easier usage\n",
    "    joined_data = [dict(zip(joined_columns, row)) for row in joined_rows]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return joined_columns, joined_data\n",
    "\n",
    "# Fetch joined data\n",
    "joined_columns, joined_data = fetch_joined_data(DB_FILE, ROW_LIMIT)\n",
    "\n",
    "# Display the joined data\n",
    "print(\"\\nJoined Data Columns:\")\n",
    "print(joined_columns)\n",
    "\n",
    "print(\"\\nJoined Data Rows:\")\n",
    "for row in joined_data:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing batch 1 to Kafka...\n",
      "Producing batch 2 to Kafka...\n",
      "Producing batch 3 to Kafka...\n",
      "Producing batch 4 to Kafka...\n",
      "Producing batch 5 to Kafka...\n",
      "Producing batch 6 to Kafka...\n",
      "Producing batch 7 to Kafka...\n",
      "Producing batch 8 to Kafka...\n",
      "Producing batch 9 to Kafka...\n",
      "Producing batch 10 to Kafka...\n",
      "Producing batch 11 to Kafka...\n",
      "Producing batch 12 to Kafka...\n",
      "Producing batch 13 to Kafka...\n",
      "Producing batch 14 to Kafka...\n",
      "Producing batch 15 to Kafka...\n",
      "Producing batch 16 to Kafka...\n",
      "Producing batch 17 to Kafka...\n",
      "Producing batch 18 to Kafka...\n",
      "Producing batch 19 to Kafka...\n",
      "Producing batch 20 to Kafka...\n",
      "Producing batch 21 to Kafka...\n",
      "Producing batch 22 to Kafka...\n",
      "Producing batch 23 to Kafka...\n",
      "Producing batch 24 to Kafka...\n",
      "Producing batch 25 to Kafka...\n",
      "Producing batch 26 to Kafka...\n",
      "Producing batch 27 to Kafka...\n",
      "Producing batch 28 to Kafka...\n",
      "Producing batch 29 to Kafka...\n",
      "Producing batch 30 to Kafka...\n",
      "Producing batch 31 to Kafka...\n",
      "Producing batch 32 to Kafka...\n",
      "Producing batch 33 to Kafka...\n",
      "Producing batch 34 to Kafka...\n",
      "Producing batch 35 to Kafka...\n",
      "Producing batch 36 to Kafka...\n",
      "Producing batch 37 to Kafka...\n",
      "Producing batch 38 to Kafka...\n",
      "Producing batch 39 to Kafka...\n",
      "Producing batch 40 to Kafka...\n",
      "Producing batch 41 to Kafka...\n",
      "Producing batch 42 to Kafka...\n",
      "Producing batch 43 to Kafka...\n",
      "Producing batch 44 to Kafka...\n",
      "Producing batch 45 to Kafka...\n",
      "Producing batch 46 to Kafka...\n",
      "Producing batch 47 to Kafka...\n",
      "Producing batch 48 to Kafka...\n",
      "Producing batch 49 to Kafka...\n",
      "Producing batch 50 to Kafka...\n",
      "Producing batch 51 to Kafka...\n",
      "Producing batch 52 to Kafka...\n",
      "Producing batch 53 to Kafka...\n",
      "Producing batch 54 to Kafka...\n",
      "Producing batch 55 to Kafka...\n",
      "Producing batch 56 to Kafka...\n",
      "Producing batch 57 to Kafka...\n",
      "Producing batch 58 to Kafka...\n",
      "Producing batch 59 to Kafka...\n",
      "Producing batch 60 to Kafka...\n",
      "Producing batch 61 to Kafka...\n",
      "Producing batch 62 to Kafka...\n",
      "Producing batch 63 to Kafka...\n",
      "Producing batch 64 to Kafka...\n",
      "Producing batch 65 to Kafka...\n",
      "Producing batch 66 to Kafka...\n",
      "Producing batch 67 to Kafka...\n",
      "Producing batch 68 to Kafka...\n",
      "Producing batch 69 to Kafka...\n",
      "Producing batch 70 to Kafka...\n",
      "Producing batch 71 to Kafka...\n",
      "Producing batch 72 to Kafka...\n",
      "Producing batch 73 to Kafka...\n",
      "Producing batch 74 to Kafka...\n",
      "Producing batch 75 to Kafka...\n",
      "Producing batch 76 to Kafka...\n",
      "Producing batch 77 to Kafka...\n",
      "Producing batch 78 to Kafka...\n",
      "Producing batch 79 to Kafka...\n",
      "Producing batch 80 to Kafka...\n",
      "Producing batch 81 to Kafka...\n",
      "Producing batch 82 to Kafka...\n",
      "Producing batch 83 to Kafka...\n",
      "Producing batch 84 to Kafka...\n",
      "Producing batch 85 to Kafka...\n",
      "Producing batch 86 to Kafka...\n",
      "Producing batch 87 to Kafka...\n",
      "Producing batch 88 to Kafka...\n",
      "Producing batch 89 to Kafka...\n",
      "Producing batch 90 to Kafka...\n",
      "Producing batch 91 to Kafka...\n",
      "Producing batch 92 to Kafka...\n",
      "Producing batch 93 to Kafka...\n",
      "Producing batch 94 to Kafka...\n",
      "Producing batch 95 to Kafka...\n",
      "Producing batch 96 to Kafka...\n",
      "Producing batch 97 to Kafka...\n",
      "Producing batch 98 to Kafka...\n",
      "Producing batch 99 to Kafka...\n",
      "Producing batch 100 to Kafka...\n",
      "Producing batch 101 to Kafka...\n",
      "Producing batch 102 to Kafka...\n",
      "Producing batch 103 to Kafka...\n",
      "Producing batch 104 to Kafka...\n",
      "Producing batch 105 to Kafka...\n",
      "Producing batch 106 to Kafka...\n",
      "Producing batch 107 to Kafka...\n",
      "Producing batch 108 to Kafka...\n",
      "Producing batch 109 to Kafka...\n",
      "Producing batch 110 to Kafka...\n",
      "Producing batch 111 to Kafka...\n",
      "Producing batch 112 to Kafka...\n",
      "Producing batch 113 to Kafka...\n",
      "Producing batch 114 to Kafka...\n",
      "Producing batch 115 to Kafka...\n",
      "Producing batch 116 to Kafka...\n",
      "Producing batch 117 to Kafka...\n",
      "Producing batch 118 to Kafka...\n",
      "Producing batch 119 to Kafka...\n",
      "Producing batch 120 to Kafka...\n",
      "Producing batch 121 to Kafka...\n",
      "Producing batch 122 to Kafka...\n",
      "Producing batch 123 to Kafka...\n",
      "Producing batch 124 to Kafka...\n",
      "Producing batch 125 to Kafka...\n",
      "Producing batch 126 to Kafka...\n",
      "Producing batch 127 to Kafka...\n",
      "Producing batch 128 to Kafka...\n",
      "Producing batch 129 to Kafka...\n",
      "Producing batch 130 to Kafka...\n",
      "Producing batch 131 to Kafka...\n",
      "Producing batch 132 to Kafka...\n",
      "Producing batch 133 to Kafka...\n",
      "Producing batch 134 to Kafka...\n",
      "Producing batch 135 to Kafka...\n",
      "Producing batch 136 to Kafka...\n",
      "Producing batch 137 to Kafka...\n",
      "Producing batch 138 to Kafka...\n",
      "Producing batch 139 to Kafka...\n",
      "Producing batch 140 to Kafka...\n",
      "Producing batch 141 to Kafka...\n",
      "Producing batch 142 to Kafka...\n",
      "Producing batch 143 to Kafka...\n",
      "Producing batch 144 to Kafka...\n",
      "Producing batch 145 to Kafka...\n",
      "Producing batch 146 to Kafka...\n",
      "Producing batch 147 to Kafka...\n",
      "Producing batch 148 to Kafka...\n",
      "Producing batch 149 to Kafka...\n",
      "Producing batch 150 to Kafka...\n",
      "Producing batch 151 to Kafka...\n",
      "Producing batch 152 to Kafka...\n",
      "Producing batch 153 to Kafka...\n",
      "Producing batch 154 to Kafka...\n",
      "Producing batch 155 to Kafka...\n",
      "Producing batch 156 to Kafka...\n",
      "Producing batch 157 to Kafka...\n",
      "Producing batch 158 to Kafka...\n",
      "Producing batch 159 to Kafka...\n",
      "Producing batch 160 to Kafka...\n",
      "Producing batch 161 to Kafka...\n",
      "Producing batch 162 to Kafka...\n",
      "Producing batch 163 to Kafka...\n",
      "Producing batch 164 to Kafka...\n",
      "Producing batch 165 to Kafka...\n",
      "Producing batch 166 to Kafka...\n",
      "Producing batch 167 to Kafka...\n",
      "Producing batch 168 to Kafka...\n",
      "Producing batch 169 to Kafka...\n",
      "Producing batch 170 to Kafka...\n",
      "Producing batch 171 to Kafka...\n",
      "Producing batch 172 to Kafka...\n",
      "Producing batch 173 to Kafka...\n",
      "Producing batch 174 to Kafka...\n",
      "Producing batch 175 to Kafka...\n",
      "Producing batch 176 to Kafka...\n",
      "Producing batch 177 to Kafka...\n",
      "Producing batch 178 to Kafka...\n",
      "Producing batch 179 to Kafka...\n",
      "Producing batch 180 to Kafka...\n",
      "Producing batch 181 to Kafka...\n",
      "Producing batch 182 to Kafka...\n",
      "Producing batch 183 to Kafka...\n",
      "Producing batch 184 to Kafka...\n",
      "Producing batch 185 to Kafka...\n",
      "Producing batch 186 to Kafka...\n",
      "Producing batch 187 to Kafka...\n",
      "Producing batch 188 to Kafka...\n",
      "Producing batch 189 to Kafka...\n",
      "Producing batch 190 to Kafka...\n",
      "Producing batch 191 to Kafka...\n",
      "Producing batch 192 to Kafka...\n",
      "Producing batch 193 to Kafka...\n",
      "Producing batch 194 to Kafka...\n",
      "Producing batch 195 to Kafka...\n",
      "Producing batch 196 to Kafka...\n",
      "Producing batch 197 to Kafka...\n",
      "Producing batch 198 to Kafka...\n",
      "Producing batch 199 to Kafka...\n",
      "Producing batch 200 to Kafka...\n",
      "Producing batch 201 to Kafka...\n",
      "Producing batch 202 to Kafka...\n",
      "Producing batch 203 to Kafka...\n",
      "Producing batch 204 to Kafka...\n",
      "Producing batch 205 to Kafka...\n",
      "Producing batch 206 to Kafka...\n",
      "Producing batch 207 to Kafka...\n",
      "Producing batch 208 to Kafka...\n",
      "Producing batch 209 to Kafka...\n",
      "Producing batch 210 to Kafka...\n",
      "Producing batch 211 to Kafka...\n",
      "Producing batch 212 to Kafka...\n",
      "Producing batch 213 to Kafka...\n",
      "Producing batch 214 to Kafka...\n",
      "Producing batch 215 to Kafka...\n",
      "Producing batch 216 to Kafka...\n",
      "Producing batch 217 to Kafka...\n",
      "Producing batch 218 to Kafka...\n",
      "Producing batch 219 to Kafka...\n",
      "Producing batch 220 to Kafka...\n",
      "Producing batch 221 to Kafka...\n",
      "Producing batch 222 to Kafka...\n",
      "Producing batch 223 to Kafka...\n",
      "Producing batch 224 to Kafka...\n",
      "Producing batch 225 to Kafka...\n",
      "Producing batch 226 to Kafka...\n",
      "Producing batch 227 to Kafka...\n",
      "Producing batch 228 to Kafka...\n",
      "Producing batch 229 to Kafka...\n",
      "Producing batch 230 to Kafka...\n",
      "Producing batch 231 to Kafka...\n",
      "Producing batch 232 to Kafka...\n",
      "Producing batch 233 to Kafka...\n",
      "Producing batch 234 to Kafka...\n",
      "Producing batch 235 to Kafka...\n",
      "Producing batch 236 to Kafka...\n",
      "Producing batch 237 to Kafka...\n",
      "Producing batch 238 to Kafka...\n",
      "Producing batch 239 to Kafka...\n",
      "Producing batch 240 to Kafka...\n",
      "Producing batch 241 to Kafka...\n",
      "Producing batch 242 to Kafka...\n",
      "Producing batch 243 to Kafka...\n",
      "Producing batch 244 to Kafka...\n",
      "Producing batch 245 to Kafka...\n",
      "Producing batch 246 to Kafka...\n",
      "Producing batch 247 to Kafka...\n",
      "Producing batch 248 to Kafka...\n",
      "Producing batch 249 to Kafka...\n",
      "Producing batch 250 to Kafka...\n",
      "Producing batch 251 to Kafka...\n",
      "Producing batch 252 to Kafka...\n",
      "Producing batch 253 to Kafka...\n",
      "Producing batch 254 to Kafka...\n",
      "Producing batch 255 to Kafka...\n",
      "Producing batch 256 to Kafka...\n",
      "Producing batch 257 to Kafka...\n",
      "Producing batch 258 to Kafka...\n",
      "Producing batch 259 to Kafka...\n",
      "Producing batch 260 to Kafka...\n",
      "Producing batch 261 to Kafka...\n",
      "Producing batch 262 to Kafka...\n",
      "Producing batch 263 to Kafka...\n",
      "Producing batch 264 to Kafka...\n",
      "Producing batch 265 to Kafka...\n",
      "Producing batch 266 to Kafka...\n",
      "Producing batch 267 to Kafka...\n",
      "Producing batch 268 to Kafka...\n",
      "Producing batch 269 to Kafka...\n",
      "Producing batch 270 to Kafka...\n",
      "Producing batch 271 to Kafka...\n",
      "Producing batch 272 to Kafka...\n",
      "Producing batch 273 to Kafka...\n",
      "Producing batch 274 to Kafka...\n",
      "Producing batch 275 to Kafka...\n",
      "Producing batch 276 to Kafka...\n",
      "Producing batch 277 to Kafka...\n",
      "Producing batch 278 to Kafka...\n",
      "Producing batch 279 to Kafka...\n",
      "Producing batch 280 to Kafka...\n",
      "Producing batch 281 to Kafka...\n",
      "Producing batch 282 to Kafka...\n",
      "Producing batch 283 to Kafka...\n",
      "Producing batch 284 to Kafka...\n",
      "Producing batch 285 to Kafka...\n",
      "Producing batch 286 to Kafka...\n",
      "Producing batch 287 to Kafka...\n",
      "Producing batch 288 to Kafka...\n",
      "Producing batch 289 to Kafka...\n",
      "Producing batch 290 to Kafka...\n",
      "Producing batch 291 to Kafka...\n",
      "Producing batch 292 to Kafka...\n",
      "Producing batch 293 to Kafka...\n",
      "Producing batch 294 to Kafka...\n",
      "Producing batch 295 to Kafka...\n",
      "Producing batch 296 to Kafka...\n",
      "Producing batch 297 to Kafka...\n",
      "Producing batch 298 to Kafka...\n",
      "Producing batch 299 to Kafka...\n",
      "Producing batch 300 to Kafka...\n",
      "Producing batch 301 to Kafka...\n",
      "Producing batch 302 to Kafka...\n",
      "Producing batch 303 to Kafka...\n",
      "Producing batch 304 to Kafka...\n",
      "Producing batch 305 to Kafka...\n",
      "Producing batch 306 to Kafka...\n",
      "Producing batch 307 to Kafka...\n",
      "Producing batch 308 to Kafka...\n",
      "Producing batch 309 to Kafka...\n",
      "Producing batch 310 to Kafka...\n",
      "Producing batch 311 to Kafka...\n",
      "Producing batch 312 to Kafka...\n",
      "Producing batch 313 to Kafka...\n",
      "Producing batch 314 to Kafka...\n",
      "Producing batch 315 to Kafka...\n",
      "Producing batch 316 to Kafka...\n",
      "Producing batch 317 to Kafka...\n",
      "Producing batch 318 to Kafka...\n",
      "Producing batch 319 to Kafka...\n",
      "Producing batch 320 to Kafka...\n",
      "Producing batch 321 to Kafka...\n",
      "Producing batch 322 to Kafka...\n",
      "Producing batch 323 to Kafka...\n",
      "Producing batch 324 to Kafka...\n",
      "Producing batch 325 to Kafka...\n",
      "Producing batch 326 to Kafka...\n",
      "Producing batch 327 to Kafka...\n",
      "Producing batch 328 to Kafka...\n",
      "Producing batch 329 to Kafka...\n",
      "Producing batch 330 to Kafka...\n",
      "Producing batch 331 to Kafka...\n",
      "Producing batch 332 to Kafka...\n",
      "Producing batch 333 to Kafka...\n",
      "Producing batch 334 to Kafka...\n",
      "Producing batch 335 to Kafka...\n",
      "Producing batch 336 to Kafka...\n",
      "Producing batch 337 to Kafka...\n",
      "Producing batch 338 to Kafka...\n",
      "Producing batch 339 to Kafka...\n",
      "Producing batch 340 to Kafka...\n",
      "Producing batch 341 to Kafka...\n",
      "Producing batch 342 to Kafka...\n",
      "Producing batch 343 to Kafka...\n",
      "Producing batch 344 to Kafka...\n",
      "Producing batch 345 to Kafka...\n",
      "Producing batch 346 to Kafka...\n",
      "Producing batch 347 to Kafka...\n",
      "Producing batch 348 to Kafka...\n",
      "Producing batch 349 to Kafka...\n",
      "Producing batch 350 to Kafka...\n",
      "Producing batch 351 to Kafka...\n",
      "Producing batch 352 to Kafka...\n",
      "Producing batch 353 to Kafka...\n",
      "Producing batch 354 to Kafka...\n",
      "Producing batch 355 to Kafka...\n",
      "Producing batch 356 to Kafka...\n",
      "Producing batch 357 to Kafka...\n",
      "Producing batch 358 to Kafka...\n",
      "Producing batch 359 to Kafka...\n",
      "Producing batch 360 to Kafka...\n",
      "Producing batch 361 to Kafka...\n",
      "Producing batch 362 to Kafka...\n",
      "Producing batch 363 to Kafka...\n",
      "Producing batch 364 to Kafka...\n",
      "Producing batch 365 to Kafka...\n",
      "Producing batch 366 to Kafka...\n",
      "Producing batch 367 to Kafka...\n",
      "Producing batch 368 to Kafka...\n",
      "Producing batch 369 to Kafka...\n",
      "Producing batch 370 to Kafka...\n",
      "Producing batch 371 to Kafka...\n",
      "Producing batch 372 to Kafka...\n",
      "Producing batch 373 to Kafka...\n",
      "Producing batch 374 to Kafka...\n",
      "Producing batch 375 to Kafka...\n",
      "Producing batch 376 to Kafka...\n",
      "Producing batch 377 to Kafka...\n",
      "Producing batch 378 to Kafka...\n",
      "Producing batch 379 to Kafka...\n",
      "Producing batch 380 to Kafka...\n",
      "Producing batch 381 to Kafka...\n",
      "Producing batch 382 to Kafka...\n",
      "Producing batch 383 to Kafka...\n",
      "Producing batch 384 to Kafka...\n",
      "Producing batch 385 to Kafka...\n",
      "Producing batch 386 to Kafka...\n",
      "Producing batch 387 to Kafka...\n",
      "Producing batch 388 to Kafka...\n",
      "Producing batch 389 to Kafka...\n",
      "Producing batch 390 to Kafka...\n",
      "Producing batch 391 to Kafka...\n",
      "Producing batch 392 to Kafka...\n",
      "Producing batch 393 to Kafka...\n",
      "Producing batch 394 to Kafka...\n",
      "Producing batch 395 to Kafka...\n",
      "Producing batch 396 to Kafka...\n",
      "Producing batch 397 to Kafka...\n",
      "Producing batch 398 to Kafka...\n",
      "Producing batch 399 to Kafka...\n",
      "Producing batch 400 to Kafka...\n",
      "Producing batch 401 to Kafka...\n",
      "Producing batch 402 to Kafka...\n",
      "Producing batch 403 to Kafka...\n",
      "Producing batch 404 to Kafka...\n",
      "Producing batch 405 to Kafka...\n",
      "Producing batch 406 to Kafka...\n",
      "Producing batch 407 to Kafka...\n",
      "Producing batch 408 to Kafka...\n",
      "Producing batch 409 to Kafka...\n",
      "Producing batch 410 to Kafka...\n",
      "Producing batch 411 to Kafka...\n",
      "Producing batch 412 to Kafka...\n",
      "Producing batch 413 to Kafka...\n",
      "Producing batch 414 to Kafka...\n",
      "Producing batch 415 to Kafka...\n",
      "Producing batch 416 to Kafka...\n",
      "Producing batch 417 to Kafka...\n",
      "Producing batch 418 to Kafka...\n",
      "Producing batch 419 to Kafka...\n",
      "Producing batch 420 to Kafka...\n",
      "Producing batch 421 to Kafka...\n",
      "Producing batch 422 to Kafka...\n",
      "Producing batch 423 to Kafka...\n",
      "Producing batch 424 to Kafka...\n",
      "Producing batch 425 to Kafka...\n",
      "Producing batch 426 to Kafka...\n",
      "Producing batch 427 to Kafka...\n",
      "Producing batch 428 to Kafka...\n",
      "Producing batch 429 to Kafka...\n",
      "Producing batch 430 to Kafka...\n",
      "Producing batch 431 to Kafka...\n",
      "Producing batch 432 to Kafka...\n",
      "Producing batch 433 to Kafka...\n",
      "Producing batch 434 to Kafka...\n",
      "Producing batch 435 to Kafka...\n",
      "Producing batch 436 to Kafka...\n",
      "Producing batch 437 to Kafka...\n",
      "Producing batch 438 to Kafka...\n",
      "Producing batch 439 to Kafka...\n",
      "Producing batch 440 to Kafka...\n",
      "Producing batch 441 to Kafka...\n",
      "Producing batch 442 to Kafka...\n",
      "Producing batch 443 to Kafka...\n",
      "Producing batch 444 to Kafka...\n",
      "Producing batch 445 to Kafka...\n",
      "Producing batch 446 to Kafka...\n",
      "Producing batch 447 to Kafka...\n",
      "Producing batch 448 to Kafka...\n",
      "Producing batch 449 to Kafka...\n",
      "Producing batch 450 to Kafka...\n",
      "Producing batch 451 to Kafka...\n",
      "Producing batch 452 to Kafka...\n",
      "Producing batch 453 to Kafka...\n",
      "Producing batch 454 to Kafka...\n",
      "Producing batch 455 to Kafka...\n",
      "Producing batch 456 to Kafka...\n",
      "Producing batch 457 to Kafka...\n",
      "Producing batch 458 to Kafka...\n",
      "Producing batch 459 to Kafka...\n",
      "Producing batch 460 to Kafka...\n",
      "Producing batch 461 to Kafka...\n",
      "Producing batch 462 to Kafka...\n",
      "Producing batch 463 to Kafka...\n",
      "Producing batch 464 to Kafka...\n",
      "Producing batch 465 to Kafka...\n",
      "Producing batch 466 to Kafka...\n",
      "Producing batch 467 to Kafka...\n",
      "Producing batch 468 to Kafka...\n",
      "Producing batch 469 to Kafka...\n",
      "Producing batch 470 to Kafka...\n",
      "Producing batch 471 to Kafka...\n",
      "Producing batch 472 to Kafka...\n",
      "Producing batch 473 to Kafka...\n",
      "Producing batch 474 to Kafka...\n",
      "Producing batch 475 to Kafka...\n",
      "Producing batch 476 to Kafka...\n",
      "Producing batch 477 to Kafka...\n",
      "Producing batch 478 to Kafka...\n",
      "Producing batch 479 to Kafka...\n",
      "Producing batch 480 to Kafka...\n",
      "Producing batch 481 to Kafka...\n",
      "Producing batch 482 to Kafka...\n",
      "Producing batch 483 to Kafka...\n",
      "Producing batch 484 to Kafka...\n",
      "Producing batch 485 to Kafka...\n",
      "Producing batch 486 to Kafka...\n",
      "Producing batch 487 to Kafka...\n",
      "Producing batch 488 to Kafka...\n",
      "Producing batch 489 to Kafka...\n",
      "Producing batch 490 to Kafka...\n",
      "Producing batch 491 to Kafka...\n",
      "Producing batch 492 to Kafka...\n",
      "Producing batch 493 to Kafka...\n",
      "Producing batch 494 to Kafka...\n",
      "Producing batch 495 to Kafka...\n",
      "Producing batch 496 to Kafka...\n",
      "Producing batch 497 to Kafka...\n",
      "Producing batch 498 to Kafka...\n",
      "Producing batch 499 to Kafka...\n",
      "Producing batch 500 to Kafka...\n",
      "Producing batch 501 to Kafka...\n",
      "Producing batch 502 to Kafka...\n",
      "Producing batch 503 to Kafka...\n",
      "Producing batch 504 to Kafka...\n",
      "Producing batch 505 to Kafka...\n",
      "Producing batch 506 to Kafka...\n",
      "Producing batch 507 to Kafka...\n",
      "Producing batch 508 to Kafka...\n",
      "Producing batch 509 to Kafka...\n",
      "Producing batch 510 to Kafka...\n",
      "Producing batch 511 to Kafka...\n",
      "Producing batch 512 to Kafka...\n",
      "Producing batch 513 to Kafka...\n",
      "Producing batch 514 to Kafka...\n",
      "Producing batch 515 to Kafka...\n",
      "Producing batch 516 to Kafka...\n",
      "Producing batch 517 to Kafka...\n",
      "Producing batch 518 to Kafka...\n",
      "Producing batch 519 to Kafka...\n",
      "Producing batch 520 to Kafka...\n",
      "Producing batch 521 to Kafka...\n",
      "Producing batch 522 to Kafka...\n",
      "Producing batch 523 to Kafka...\n",
      "Producing batch 524 to Kafka...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Main Producer Logic\u001b[39;00m\n\u001b[0;32m     53\u001b[0m producer \u001b[38;5;241m=\u001b[39m create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n\u001b[1;32m---> 54\u001b[0m \u001b[43mingest_and_produce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTABLE_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproducer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKAFKA_TOPIC\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mingest_and_produce\u001b[1;34m(db_file, table_name, producer, kafka_topic, batch_size)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProducing batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to Kafka...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m     send_to_kafka(producer, kafka_topic, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, value\u001b[38;5;241m=\u001b[39mserialized_batch_json)\n\u001b[1;32m---> 46\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Optional: Add a delay to control throughput\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData ingestion and production completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m cursor\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "\n",
    "# SQLite Configuration\n",
    "DB_FILE = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "TABLE_NAME = 'indicators'  # Replace with your table name\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "KAFKA_TOPIC = 'indicators-stream'\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "# Send data to Kafka\n",
    "def send_to_kafka(producer, topic, key, value):\n",
    "    producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()\n",
    "\n",
    "# Ingest data from SQLite and produce to Kafka\n",
    "def ingest_and_produce(db_file, table_name, producer, kafka_topic, batch_size=100):\n",
    "    # Connect to SQLite\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch data from the specified table\n",
    "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Get column names for JSON serialization\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Process and produce data in batches\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        batch = rows[i:i + batch_size]\n",
    "        serialized_batch = [\n",
    "            dict(zip(column_names, row)) for row in batch\n",
    "        ]  # Convert to JSON-serializable format\n",
    "        serialized_batch_json = json.dumps(serialized_batch)\n",
    "\n",
    "        print(f\"Producing batch {i // batch_size + 1} to Kafka...\")\n",
    "        send_to_kafka(producer, kafka_topic, key=None, value=serialized_batch_json)\n",
    "        time.sleep(1)  # Optional: Add a delay to control throughput\n",
    "\n",
    "    print(\"Data ingestion and production completed.\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Main Producer Logic\n",
    "producer = create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n",
    "ingest_and_produce(DB_FILE, TABLE_NAME, producer, KAFKA_TOPIC)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m db_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Connect to the SQLite database\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create a cursor object to interact with the database\u001b[39;00m\n\u001b[0;32m     10\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "\u001b[1;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_file = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Example: List all tables in the database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Print all table names\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Example: Read data from a specific table (replace 'your_table_name' with an actual table name)\n",
    "table_name = 'pulses'  # Replace with your actual table name\n",
    "cursor.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "\n",
    "# Fetch all rows from the query result\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Print the data\n",
    "print(f\"Data from {table_name}:\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and the connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the table pulses:\n",
      "Column name: id\n",
      "Column name: name\n",
      "Column name: description\n",
      "Column name: author_name\n",
      "Column name: modified\n",
      "Column name: created\n",
      "Column name: public\n",
      "Column name: adversary\n",
      "Column name: TLP\n",
      "Column name: revision\n",
      "Column name: in_group\n",
      "Column name: is_subscribing\n",
      "Column name: malware_family\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_file = r\"C:\\Users\\I745988\\Downloads\\2023-10-25_cti_data_majd\\2023-10-25_cti_data_majd.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Name of the table you want to check\n",
    "table_name = 'pulses'  # Replace with the actual table name\n",
    "\n",
    "# Execute PRAGMA command to get the columns of the table\n",
    "cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "\n",
    "# Fetch all rows from the query result\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "# Print the column details (column_name is the 2nd element in the tuple)\n",
    "print(f\"Columns in the table {table_name}:\")\n",
    "for column in columns:\n",
    "    print(f\"Column name: {column[1]}\")\n",
    "\n",
    "# Close the cursor and connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the table indicators:\n",
      "Column name: id\n",
      "Column name: pulse_id\n",
      "Column name: indicator\n",
      "Column name: type\n",
      "Column name: created\n",
      "Column name: content\n",
      "Column name: title\n",
      "Column name: description\n",
      "Column name: expiration\n",
      "Column name: is_active\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_file = r\"C:\\Users\\I745988\\Downloads\\2023-10-25_cti_data_majd\\2023-10-25_cti_data_majd.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Name of the table you want to check\n",
    "table_name = 'indicators'  # Replace with the actual table name\n",
    "\n",
    "# Execute PRAGMA command to get the columns of the table\n",
    "cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "\n",
    "# Fetch all rows from the query result\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "# Print the column details (column_name is the 2nd element in the tuple)\n",
    "print(f\"Columns in the table {table_name}:\")\n",
    "for column in columns:\n",
    "    print(f\"Column name: {column[1]}\")\n",
    "\n",
    "# Close the cursor and connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the table ip_location:\n",
      "Column name: ip\n",
      "Column name: cityName\n",
      "Column name: countryName\n",
      "Column name: latitude\n",
      "Column name: longitude\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_file = r\"C:\\Users\\I745988\\Downloads\\2023-10-25_cti_data_majd\\2023-10-25_cti_data_majd.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Name of the table you want to check\n",
    "table_name = 'ip_location'  # Replace with the actual table name\n",
    "\n",
    "# Execute PRAGMA command to get the columns of the table\n",
    "cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "\n",
    "# Fetch all rows from the query result\n",
    "columns = cursor.fetchall()\n",
    "\n",
    "# Print the column details (column_name is the 2nd element in the tuple)\n",
    "print(f\"Columns in the table {table_name}:\")\n",
    "for column in columns:\n",
    "    print(f\"Column name: {column[1]}\")\n",
    "\n",
    "# Close the cursor and connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "future = producer.send('indicators-stream', b'Test message')\n",
    "\n",
    "try:\n",
    "    record_metadata = future.get(timeout=10)  # Wait for response to confirm the message\n",
    "    print(f\"Message sent to topic {record_metadata.topic} partition {record_metadata.partition}\")\n",
    "except KafkaError as e:\n",
    "    print(f\"Error sending message: {e}\")\n",
    "finally:\n",
    "    producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Function to initialize Kafka producer\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "# Function to send data to Kafka\n",
    "def send_to_kafka(producer, topic, key, value):\n",
    "    producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()  # Make sure to flush after sending\n",
    "\n",
    "# Function to process and send indicators in batches\n",
    "def process_and_send_to_kafka(indicators, producer, topic, batch_size=100):\n",
    "    for i in range(0, len(indicators), batch_size):\n",
    "        batch = indicators[i:i + batch_size]\n",
    "        serialized_batch = json.dumps(batch)  # Serialize batch to JSON\n",
    "        print(f\"Sending batch {i // batch_size + 1} to Kafka...\")\n",
    "        send_to_kafka(producer, topic, key=None, value=serialized_batch)\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'  # Confluent Kafka broker address\n",
    "KAFKA_TOPIC = 'indicators-stream'  # Correct topic name where messages should be sent\n",
    "\n",
    "# Create Kafka producer\n",
    "producer = create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "# Example indicator data (for testing)\n",
    "indicators = [{'id': i, 'value': f'Indicator {i}'} for i in range(500)]  # Simulate 500 indicators\n",
    "\n",
    "# Process and send indicators to Kafka\n",
    "CHUNK_SIZE = 500  # Number of indicators per batch\n",
    "limited_indicators = indicators[:CHUNK_SIZE]\n",
    "print(f\"Processing and sending the first {len(limited_indicators)} indicators to Kafka...\")\n",
    "\n",
    "process_and_send_to_kafka(limited_indicators, producer, KAFKA_TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "import json\n",
    "\n",
    "# Kafka Consumer Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Your broker's address\n",
    "    'group.id': 'test-consumer-group',          # Consumer group ID\n",
    "    'auto.offset.reset': 'earliest'             # Consume from the earliest message if no offset exists\n",
    "}\n",
    "\n",
    "# Initialize Kafka Consumer\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Subscribe to the topic\n",
    "consumer.subscribe(['indicators-stream'])\n",
    "\n",
    "try:\n",
    "    print(\"Consuming messages from 'indicators-stream'...\")\n",
    "    while True:\n",
    "        # Poll for new messages\n",
    "        msg = consumer.poll(timeout=1.0)  # Wait for up to 1 second for a message\n",
    "        if msg is None:\n",
    "            continue  # No message received\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(f\"End of partition reached {msg.topic()} {msg.partition()} offset {msg.offset()}\")\n",
    "            else:\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # Decode and parse the message\n",
    "            raw_message = msg.value().decode('utf-8')\n",
    "            print(f\"Raw Message: {raw_message}\")\n",
    "            \n",
    "            try:\n",
    "                # Attempt to parse the message as JSON\n",
    "                parsed_message = json.loads(raw_message)\n",
    "                print(\"Parsed Message (JSON):\")\n",
    "                print(json.dumps(parsed_message, indent=4))  # Pretty print JSON\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Message is not in JSON format.\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nConsumption interrupted by user.\")\n",
    "finally:\n",
    "    print(\"Closing consumer...\")\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching first 10 rows from table 'pulses'...\n",
      "Columns in 'pulses': ['id', 'name', 'description', 'author_name', 'modified', 'created', 'public', 'adversary', 'TLP', 'revision', 'in_group', 'is_subscribing', 'malware_family']\n",
      "{'id': '645c9a033dd0e4e34e6cba8f', 'name': 'Malware Bazaar 7', 'description': '', 'author_name': 'LoveAndren', 'modified': '2023-05-11T07:32:19.907000', 'created': '2023-05-11T07:32:19.907000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63ac057b2644355f3788973e', 'name': 'Cyble - New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': 'A look at some of the latest ransomware strains created based on the leaked source code of Conti ransomware, which was discovered in 2021 and is being investigated by Cyble Research and Intelligence Labs (CRIL).', 'author_name': 'CyberHunter_NL', 'modified': '2022-12-28T08:59:38.883000', 'created': '2022-12-28T08:59:38.883000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63abba430849f35bb074ab1a', 'name': 'Cyble: New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': '', 'author_name': 'goatluxy', 'modified': '2022-12-28T03:38:43.688000', 'created': '2022-12-28T03:38:43.688000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a55b8feb56dde69cb1e884', 'name': 'New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': 'New ransomware strains are emerging based on the leaked source code of Conti ransomware, Cyble Research and Intelligence Labs (CRIL) has discovered, along with a new strain named Putin Team and BlueSky Meow.', 'author_name': 'AlienVault', 'modified': '2022-12-23T07:41:02.221000', 'created': '2022-12-23T07:41:02.221000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a54a24d28f6f73643c7aa0', 'name': 'Cyble &mdash; New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': '', 'author_name': 'Tr1sa111', 'modified': '2022-12-23T06:26:44.410000', 'created': '2022-12-23T06:26:44.410000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a549d1845511f75f35d875', 'name': 'Cyble &mdash; New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': '', 'author_name': 'tr2222200', 'modified': '2022-12-23T06:25:21.262000', 'created': '2022-12-23T06:25:21.262000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a549c76a08a43303915377', 'name': 'Cyble &mdash; New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': 'New ransomware strains are emerging based on the leaked source code of Conti ransomware, Cyble Research and Intelligence Labs (CRIL) has discovered, along with a new strain named Putin Team and BlueSky Meow.', 'author_name': 'tr2222200', 'modified': '2022-12-23T06:25:11.417000', 'created': '2022-12-23T06:25:11.417000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a53c463b8a78fd4aafb952', 'name': 'Cyble &mdash; New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': '', 'author_name': 'cyberasmi', 'modified': '2022-12-23T05:27:34.886000', 'created': '2022-12-23T05:27:34.886000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a4bf71b46e295f7c5c664e', 'name': 'New Ransomware Strains Emerging From Leaked Conti’s Source Code ', 'description': '', 'author_name': 'feisty-swim1410', 'modified': '2022-12-22T20:34:57.730000', 'created': '2022-12-22T20:34:57.730000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "{'id': '63a481d1b821012188699107', 'name': 'New Ransomware Strains Emerging from Leaked Conti’s Source Code', 'description': 'New ransomware strains are emerging based on the leaked source code of Conti ransomware, Cyble Research and Intelligence Labs (CRIL) has discovered, along with a new strain named Putin Team and BlueSky Meow.', 'author_name': 'AlienVault', 'modified': '2022-12-22T16:12:00.733000', 'created': '2022-12-22T16:12:00.733000', 'public': 1, 'adversary': '', 'TLP': 'white', 'revision': 1, 'in_group': 0, 'is_subscribing': None, 'malware_family': '%23Exploit:NtQueryIntervalProfile'}\n",
      "\n",
      "Producing rows from 'pulses' to Kafka...\n",
      "\n",
      "Fetching first 10 rows from table 'indicators'...\n",
      "Columns in 'indicators': ['id', 'pulse_id', 'indicator', 'type', 'created', 'content', 'title', 'description', 'expiration', 'is_active']\n",
      "{'id': 3678952388, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '32788cd0818804797beb15135695e165', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 62958d20e3f6b262d16e5d5478c2c0d882ecec72b1e7d26276c07d7187dc0a1e', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952389, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '32dd32a4496e689ba904e92385d350d5', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 82521006cdb75d3826edb12e2d7a038a37cdafddf220cac4ae927596491c9421', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952390, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '3343fe2b2f76cda4c776eb592a4d0146', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Win32/Flooder.Agent', 'description': 'MD5 of 3c4c8fdda37010277359be7d34620f97ac7485270c73568471603913097ffb96', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952391, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '48e074289337a25992d16166bab58c73', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 40b3e23941cb1a50f8061c5feee2ce505833f2b8269ec402018359bf1a399b20', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952392, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '623b083c79bd9855db70cb1509861cb0', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 3ed019075072f4b6744c45753a17e5210bf33405a25c0c285b9e302d341f5210', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952393, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '6654235619b25fd70f0233aec854e0ea', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Arechclient2', 'description': 'MD5 of 83956ff42ffb6c352e3257a55ab76416b120a1cbf6b1f92672d50c4b8fae404f', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952394, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '6cbe61e619d1c2d892e95c7dac37582f', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 3cea6c13531dbf5b2fdd0a3aca1c09bf8efa53d37d6116e13791cac1296540c5', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952395, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '74b7fa40d5864c398a9210c6ef7fed06', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 4a29128a2d1f0bce07b0a16c41d3ceed6354dffe4d22fa526238cd7b74233433', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952396, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '7d805076b1ccffc8a34ca42506dd9a57', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Arechclient2', 'description': 'MD5 of 875972c6043bdc3ac7cc51ac8f7ae58a43b43fcfc933672caa9a0f92f27825c0', 'expiration': None, 'is_active': 1}\n",
      "{'id': 3678952397, 'pulse_id': '645c9a033dd0e4e34e6cba8f', 'indicator': '969a55f86f1afbbdf9e823be4325ffff', 'type': 'FileHash-MD5', 'created': '2023-05-11T07:32:46', 'content': '', 'title': 'Bdaejec, Flooder.LYI, Win32/Flooder.Agent', 'description': 'MD5 of 45a2970a08952954afcbbb120619f5c0736643a691d26d2563604139fbab180f', 'expiration': None, 'is_active': 1}\n",
      "\n",
      "Producing rows from 'indicators' to Kafka...\n",
      "\n",
      "Fetching first 10 rows from table 'ip_location'...\n",
      "Columns in 'ip_location': ['ip', 'cityName', 'countryName', 'latitude', 'longitude']\n",
      "{'ip': '148.251.234.93', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.162.83', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.189.114', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.208.18', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.40.177', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.104.67', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.110.152', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.147.72', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.184.189', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'ip': '148.251.20.250', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "\n",
      "Producing rows from 'ip_location' to Kafka...\n",
      "\n",
      "Data ingestion and production completed.\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# SQLite Configuration\n",
    "DB_FILE = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "TABLES = ['pulses', 'indicators', 'ip_location']  # List of table names\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "KAFKA_TOPIC = 'data-stream'\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "# Send data to Kafka\n",
    "def send_to_kafka(producer, topic, key, value):\n",
    "    producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()\n",
    "\n",
    "# Display and ingest data from SQLite\n",
    "def display_and_ingest_data(db_file, tables, producer, kafka_topic, row_limit=10):\n",
    "    # Connect to SQLite\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Loop through each table\n",
    "    for table_name in tables:\n",
    "        print(f\"\\nFetching first {row_limit} rows from table '{table_name}'...\")\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT {row_limit}\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Get column names for better display and JSON serialization\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        \n",
    "        # Display the rows\n",
    "        print(f\"Columns in '{table_name}': {column_names}\")\n",
    "        for row in rows:\n",
    "            print(dict(zip(column_names, row)))\n",
    "\n",
    "        # Serialize and produce each row to Kafka\n",
    "        print(f\"\\nProducing rows from '{table_name}' to Kafka...\")\n",
    "        for row in rows:\n",
    "            serialized_row = json.dumps(dict(zip(column_names, row)))  # Convert to JSON\n",
    "            send_to_kafka(producer, kafka_topic, key=None, value=serialized_row)\n",
    "\n",
    "    print(\"\\nData ingestion and production completed.\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Main Producer Logic\n",
    "producer = create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n",
    "display_and_ingest_data(DB_FILE, TABLES, producer, KAFKA_TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Joined Data Columns:\n",
      "['indicator_id', 'pulse_id', 'indicator', 'type', 'created', 'title', 'description', 'cityName', 'countryName', 'latitude', 'longitude']\n",
      "\n",
      "Joined Data Rows:\n",
      "{'indicator_id': 2233484595, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.234.93', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3762239076, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.162.83', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3748688316, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.189.114', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3462478129, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.208.18', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3628506579, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.40.177', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3747596924, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.104.67', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 2239778756, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.110.152', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3761839080, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.147.72', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3761839089, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.184.189', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n",
      "{'indicator_id': 3663336263, 'pulse_id': '6518f9615a88e0f1e325bde4', 'indicator': '148.251.20.250', 'type': 'IPv4', 'created': '2023-10-01T04:45:24', 'title': '', 'description': 'CC=DE ASN=AS24940 Hetzner Online GmbH', 'cityName': 'Berlin', 'countryName': 'Germany', 'latitude': '52.524525', 'longitude': '13.410037'}\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# SQLite Configuration\n",
    "DB_FILE = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "ROW_LIMIT = 10  # Limit the number of rows fetched\n",
    "\n",
    "# Perform the join operation between 'indicators' and 'ip_location'\n",
    "def fetch_joined_data(db_file, row_limit):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    join_query = \"\"\"\n",
    "    SELECT \n",
    "        ind.id AS indicator_id,\n",
    "        ind.pulse_id,\n",
    "        ind.indicator,\n",
    "        ind.type,\n",
    "        ind.created,\n",
    "        ind.title,\n",
    "        ind.description,\n",
    "        loc.cityName,\n",
    "        loc.countryName,\n",
    "        loc.latitude,\n",
    "        loc.longitude\n",
    "    FROM \n",
    "        indicators AS ind\n",
    "    INNER JOIN \n",
    "        ip_location AS loc\n",
    "    ON \n",
    "        ind.indicator = loc.ip\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    cursor.execute(join_query, (row_limit,))\n",
    "    joined_rows = cursor.fetchall()\n",
    "\n",
    "    # Get column names for joined data\n",
    "    joined_columns = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Convert rows to dictionary for easier usage\n",
    "    joined_data = [dict(zip(joined_columns, row)) for row in joined_rows]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return joined_columns, joined_data\n",
    "\n",
    "# Fetch joined data\n",
    "joined_columns, joined_data = fetch_joined_data(DB_FILE, ROW_LIMIT)\n",
    "\n",
    "# Display the joined data\n",
    "print(\"\\nJoined Data Columns:\")\n",
    "print(joined_columns)\n",
    "\n",
    "print(\"\\nJoined Data Rows:\")\n",
    "for row in joined_data:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "\n",
    "# SQLite Configuration\n",
    "DB_FILE = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "TABLE_NAME = 'indicators'  # Replace with your table name\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "KAFKA_TOPIC = 'indicators-stream'\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "# Send data to Kafka\n",
    "def send_to_kafka(producer, topic, key, value):\n",
    "    producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()\n",
    "\n",
    "# Ingest data from SQLite and produce to Kafka\n",
    "def ingest_and_produce(db_file, table_name, producer, kafka_topic, batch_size=100):\n",
    "    # Connect to SQLite\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch data from the specified table\n",
    "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Get column names for JSON serialization\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Process and produce data in batches\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        batch = rows[i:i + batch_size]\n",
    "        serialized_batch = [\n",
    "            dict(zip(column_names, row)) for row in batch\n",
    "        ]  # Convert to JSON-serializable format\n",
    "        serialized_batch_json = json.dumps(serialized_batch)\n",
    "\n",
    "        print(f\"Producing batch {i // batch_size + 1} to Kafka...\")\n",
    "        send_to_kafka(producer, kafka_topic, key=None, value=serialized_batch_json)\n",
    "        time.sleep(1)  # Optional: Add a delay to control throughput\n",
    "\n",
    "    print(\"Data ingestion and production completed.\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Main Producer Logic\n",
    "producer = create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n",
    "ingest_and_produce(DB_FILE, TABLE_NAME, producer, KAFKA_TOPIC)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import logging\n",
    "from confluent_kafka import Producer, Consumer, KafkaException, KafkaError\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "import logging\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "KAFKA_TOPIC = 'indicators-stream'\n",
    "CONSUMER_GROUP_ID = 'test-consumer-group'\n",
    "\n",
    "# SQLite Configuration\n",
    "DB_FILE = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "TEMP_FILE = \"joined_data.jsonl\"  # Temporary file to store the joined data\n",
    "ROW_CHUNK_SIZE = 100  # Number of rows to send per Kafka batch\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Kafka Producer Initialization\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    logging.info(\"Initializing Kafka producer.\")\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "\n",
    "\n",
    "def ensure_kafka_topic(bootstrap_servers, topic_name):\n",
    "    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers, client_id=\"topic_creator\")\n",
    "    try:\n",
    "        topics = admin_client.list_topics()\n",
    "        if topic_name in topics:\n",
    "            logging.info(f\"Kafka topic '{topic_name}' already exists.\")\n",
    "        else:\n",
    "            logging.info(f\"Creating Kafka topic '{topic_name}'...\")\n",
    "            new_topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1)\n",
    "            admin_client.create_topics(new_topics=[new_topic])\n",
    "            logging.info(f\"Topic '{topic_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while ensuring topic existence: {e}\")\n",
    "    finally:\n",
    "        admin_client.close()\n",
    "\n",
    "\n",
    "# Fetch all data from SQLite and save it to a file\n",
    "def save_joined_data_to_file(db_file, output_file):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    join_query = \"\"\"\n",
    "    SELECT \n",
    "        ind.id AS indicator_id,\n",
    "        ind.pulse_id,\n",
    "        ind.indicator,\n",
    "        ind.type,\n",
    "        ind.created AS indicator_created,\n",
    "        ind.content,\n",
    "        ind.title AS indicator_title,\n",
    "        ind.description AS indicator_description,\n",
    "        ind.expiration,\n",
    "        ind.is_active,\n",
    "        loc.cityName,\n",
    "        loc.countryName,\n",
    "        loc.latitude,\n",
    "        loc.longitude,\n",
    "        puls.name AS pulse_name,\n",
    "        puls.description AS pulse_description,\n",
    "        puls.author_name,\n",
    "        puls.malware_family\n",
    "    FROM \n",
    "        indicators AS ind\n",
    "    INNER JOIN \n",
    "        ip_location AS loc ON ind.indicator = loc.ip\n",
    "    INNER JOIN \n",
    "        pulses AS puls ON ind.pulse_id = puls.id;\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Fetching and joining data from the database.\")\n",
    "    cursor.execute(join_query)\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    if not rows:\n",
    "        logging.warning(\"No data found in the database.\")\n",
    "        return\n",
    "\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for row in rows:\n",
    "            json_record = json.dumps(dict(zip(column_names, row)))\n",
    "            file.write(json_record + \"\\n\")\n",
    "\n",
    "    logging.info(f\"Joined data saved to {output_file} with {len(rows)} records.\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Stream the saved data to Kafka in batches\n",
    "def stream_data_to_kafka(producer, kafka_topic, input_file, chunk_size):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        buffer = []\n",
    "        record_count = 0\n",
    "\n",
    "        for line in file:\n",
    "            buffer.append((None, line.strip()))\n",
    "            record_count += 1\n",
    "\n",
    "            if len(buffer) >= chunk_size:\n",
    "                logging.info(f\"Sending chunk of {len(buffer)} records to Kafka.\")\n",
    "                send_to_kafka_batch(producer, kafka_topic, buffer)\n",
    "                logging.info(f\"Successfully sent chunk of {len(buffer)} records.\")\n",
    "                buffer = []\n",
    "\n",
    "        if buffer:\n",
    "            logging.info(f\"Sending final chunk of {len(buffer)} records to Kafka.\")\n",
    "            send_to_kafka_batch(producer, kafka_topic, buffer)\n",
    "            logging.info(f\"Successfully sent final chunk of {len(buffer)} records.\")\n",
    "\n",
    "    logging.info(f\"Total records streamed to Kafka: {record_count}\")\n",
    "\n",
    "# Send a batch of messages to Kafka\n",
    "def send_to_kafka_batch(producer, topic, messages):\n",
    "    for key, value in messages:\n",
    "        producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()\n",
    "\n",
    "# Kafka Consumer Initialization\n",
    "def create_kafka_consumer(bootstrap_servers, group_id):\n",
    "    return Consumer({\n",
    "        'bootstrap.servers': bootstrap_servers,\n",
    "        'group.id': group_id,\n",
    "        'auto.offset.reset': 'earliest'\n",
    "    })\n",
    "\n",
    "# Consume messages from Kafka\n",
    "def consume_from_kafka(consumer, topic):\n",
    "    consumer.subscribe([topic])\n",
    "    try:\n",
    "        logging.info(f\"Consuming messages from topic '{topic}'...\")\n",
    "        while True:\n",
    "            msg = consumer.poll(timeout=1.0)\n",
    "            if msg is None:\n",
    "                continue\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    logging.info(f\"End of partition reached {msg.topic()} {msg.partition()} offset {msg.offset()}\")\n",
    "                else:\n",
    "                    raise KafkaException(msg.error())\n",
    "            else:\n",
    "                raw_message = msg.value().decode('utf-8')\n",
    "                logging.info(f\"Received Message: {raw_message}\")\n",
    "                try:\n",
    "                    parsed_message = json.loads(raw_message)\n",
    "                    logging.info(\"Parsed Message (JSON):\")\n",
    "                    logging.info(json.dumps(parsed_message, indent=4))\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.warning(\"Message is not in JSON format.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Consumption interrupted by user.\")\n",
    "    finally:\n",
    "        logging.info(\"Closing Kafka consumer.\")\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in c:\\users\\i745988\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\I745988\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\I745988\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\I745988\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\I745988\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 17:58:47,980 - INFO - Fetching and displaying joined data...\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from confluent_kafka import Producer, Consumer, KafkaException, KafkaError\n",
    "\n",
    "# --- Configuration ---\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'  # Kafka broker address\n",
    "KAFKA_TOPIC = 'indicators-stream'  # Kafka topic for streaming data\n",
    "DB_FILE = r\"C:/Users/I745988/Downloads/2023-10-25_cti_data_majd/2023-10-25_cti_data_majd.db\"\n",
    "TABLES = ['indicators', 'ip_location']  # List of tables to process\n",
    "ROW_LIMIT = 10000000  # Limit on rows for each table fetch\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# --- Kafka Producer Setup ---\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "def send_to_kafka(producer, topic, key, value):\n",
    "    producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()\n",
    "\n",
    "# --- Data Fetching and Processing from SQLite ---\n",
    "def fetch_joined_data(db_file, row_limit):\n",
    "    \"\"\"Fetch data by joining 'indicators' and 'ip_location' tables\"\"\"\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    join_query = \"\"\"\n",
    "    SELECT \n",
    "        ind.id AS indicator_id,\n",
    "        ind.pulse_id,\n",
    "        ind.indicator,\n",
    "        ind.type,\n",
    "        ind.created,\n",
    "        ind.title,\n",
    "        ind.description,\n",
    "        loc.cityName,\n",
    "        loc.countryName,\n",
    "        loc.latitude,\n",
    "        loc.longitude\n",
    "    FROM \n",
    "        indicators AS ind\n",
    "    INNER JOIN \n",
    "        ip_location AS loc\n",
    "    ON \n",
    "        ind.indicator = loc.ip\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    cursor.execute(join_query, (row_limit,))\n",
    "    joined_rows = cursor.fetchall()\n",
    "\n",
    "    # Get column names for joined data\n",
    "    joined_columns = [description[0] for description in cursor.description]\n",
    "\n",
    "    # Convert rows to dictionary for easier usage\n",
    "    joined_data = [dict(zip(joined_columns, row)) for row in joined_rows]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return joined_columns, joined_data\n",
    "\n",
    "def display_and_ingest_data(db_file, tables, producer, kafka_topic, row_limit=10000000000):\n",
    "    \"\"\"Ingest data from specified tables and produce to Kafka\"\"\"\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for table_name in tables:\n",
    "        logging.info(f\"Fetching first {row_limit} rows from table '{table_name}'...\")\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT {row_limit}\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Get column names for JSON serialization\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "        logging.info(f\"Columns in '{table_name}': {column_names}\")\n",
    "        for row in rows:\n",
    "            logging.info(dict(zip(column_names, row)))\n",
    "\n",
    "        logging.info(f\"\\nProducing rows from '{table_name}' to Kafka...\")\n",
    "        for row in rows:\n",
    "            serialized_row = json.dumps(dict(zip(column_names, row)))  # Convert to JSON\n",
    "            send_to_kafka(producer, kafka_topic, key=None, value=serialized_row)\n",
    "\n",
    "    logging.info(\"Data ingestion and production completed.\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# --- Kafka Consumer Setup ---\n",
    "def create_kafka_consumer(bootstrap_servers, group_id):\n",
    "    return Consumer({\n",
    "        'bootstrap.servers': bootstrap_servers,\n",
    "        'group.id': group_id,\n",
    "        'auto.offset.reset': 'earliest'\n",
    "    })\n",
    "\n",
    "def consume_from_kafka(consumer, topic):\n",
    "    consumer.subscribe([topic])\n",
    "    try:\n",
    "        logging.info(f\"Consuming messages from topic '{topic}'...\")\n",
    "        while True:\n",
    "            msg = consumer.poll(timeout=1.0)  # Wait for up to 1 second for a message\n",
    "            if msg is None:\n",
    "                continue  # No message received\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    logging.info(f\"End of partition reached {msg.topic()} {msg.partition()} offset {msg.offset()}\")\n",
    "                else:\n",
    "                    raise KafkaException(msg.error())\n",
    "            else:\n",
    "                # Decode and parse the message\n",
    "                raw_message = msg.value().decode('utf-8')\n",
    "                logging.info(f\"Raw Message: {raw_message}\")\n",
    "\n",
    "                try:\n",
    "                    # Attempt to parse the message as JSON\n",
    "                    parsed_message = json.loads(raw_message)\n",
    "                    logging.info(\"Parsed Message (JSON):\")\n",
    "                    logging.info(json.dumps(parsed_message, indent=4))  # Pretty print JSON\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.error(\"Message is not in JSON format.\")\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"\\nConsumption interrupted by user.\")\n",
    "    finally:\n",
    "        logging.info(\"Closing consumer...\")\n",
    "        consumer.close()\n",
    "\n",
    "# --- Main Workflow ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Kafka producer\n",
    "    producer = create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "    # Fetch and display joined data from the database\n",
    "    logging.info(\"Fetching and displaying joined data...\")\n",
    "    joined_columns, joined_data = fetch_joined_data(DB_FILE, ROW_LIMIT)\n",
    "    logging.info(\"Joined Data Columns:\")\n",
    "    logging.info(joined_columns)\n",
    "    logging.info(\"Joined Data Rows:\")\n",
    "    for row in joined_data:\n",
    "        logging.info(row)\n",
    "\n",
    "    # Ingest and produce data to Kafka using the existing function\n",
    "    logging.info(\"Ingesting and producing data to Kafka...\")\n",
    "    display_and_ingest_data(DB_FILE, TABLES, producer, KAFKA_TOPIC, ROW_LIMIT)\n",
    "\n",
    "    \"\"\"# Initialize Kafka consumer to consume from the same topic\n",
    "    consumer = create_kafka_consumer(KAFKA_BOOTSTRAP_SERVERS, 'test-consumer-group')\n",
    "    consume_from_kafka(consumer, KAFKA_TOPIC)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logstash_config_pipeline_1 = \"\"\"\n",
    "input {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topics => [\"indicators-stream\"]\n",
    "    group_id => \"top-10-target-country-consumer\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  if [countryName] {\n",
    "    mutate {\n",
    "      add_field => { \"target_country\" => \"%{countryName}\" } \n",
    "    }\n",
    "    mutate {\n",
    "      add_field => { \"count\" => 1 }  \n",
    "    }\n",
    "  } else {\n",
    "    drop { } \n",
    "  }\n",
    "}\n",
    "\n",
    "aggregate {\n",
    "  task_id => \"%{target_country}\" \n",
    "  code => \"\n",
    "    map['count'] ||= 0\n",
    "    map['count'] += event.get('count').to_i \n",
    "  \"\n",
    "  push_previous_map_as_event => true\n",
    "  timeout => 60 \n",
    "}\n",
    "\n",
    "ruby {\n",
    "  code => \"\n",
    "    event.set('sorted_countries', event.get('aggregate_maps').values.sort_by { |entry| -entry['count'] }.first(10))\n",
    "  \"\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topic_id => \"top-target-countries\"  \n",
    "    codec => \"json\"\n",
    "  }\n",
    "  stdout {\n",
    "    codec => rubydebug  \n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "# Step 2: Write Pipeline 1 config to a file\n",
    "logstash_config_path_1 = r\"C:\\logstash-8.16.1\\pipeline_1.conf\"\n",
    "with open(logstash_config_path_1, 'w', encoding='utf-8') as f:\n",
    "    f.write(logstash_config_pipeline_1)\n",
    "print(\"Pipeline 1 configuration written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logstash_config_pipeline_2 = \"\"\"\n",
    "input {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topics => [\"indicators-stream\"]\n",
    "    group_id => \"top-10-threat-source-consumer\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  if [countryName] {\n",
    "    mutate {\n",
    "      add_field => { \"source_country\" => \"%{countryName}\" }  \n",
    "    }\n",
    "    mutate {\n",
    "      add_field => { \"count\" => 1 } \n",
    "    }\n",
    "  } else {\n",
    "    drop { } \n",
    "  }\n",
    "}\n",
    "\n",
    "aggregate {\n",
    "  task_id => \"%{source_country}\"\n",
    "  code => \"\n",
    "    map['count'] ||= 0\n",
    "    map['count'] += event.get('count').to_i\n",
    "  \"\n",
    "  push_previous_map_as_event => true\n",
    "  timeout => 60f\n",
    "}\n",
    "\n",
    "ruby {\n",
    "  code => \"\n",
    "    event.set('sorted_source_countries', event.get('aggregate_maps').values.sort_by { |entry| -entry['count'] }.first(10))\n",
    "  \"\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topic_id => \"top-threat-source-countries\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "  stdout {\n",
    "    codec => rubydebug\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "# Step 2: Write Pipeline 2 config to a file\n",
    "logstash_config_path_2 = r\"C:\\logstash-8.16.1\\pipeline_2.conf\"\n",
    "with open(logstash_config_path_2, 'w', encoding='utf-8') as f:\n",
    "    f.write(logstash_config_pipeline_2)\n",
    "print(\"Pipeline 2 configuration written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logstash_config_pipeline_3 = \"\"\"\n",
    "input {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topics => [\"indicators-stream\"]\n",
    "    group_id => \"detect-target-changes-consumer\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  if [countryName] {\n",
    "    mutate {\n",
    "      add_field => { \"target_country\" => \"%{countryName}\" }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if ([target_country] != [previous_target_country]) {\n",
    "    mutate {\n",
    "      add_field => { \"country_change\" => \"true\" }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  mutate {\n",
    "    add_field => { \"previous_target_country\" => \"%{target_country}\" }\n",
    "  }\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topic_id => \"target-country-change-report\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "  stdout {\n",
    "    codec => rubydebug\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "# Step 2: Write Pipeline 3 config to a file\n",
    "logstash_config_path_3 = r\"C:\\logstash-8.16.1\\pipeline_3.conf\"\n",
    "with open(logstash_config_path_3, 'w', encoding='utf-8') as f:\n",
    "    f.write(logstash_config_pipeline_3)\n",
    "print(\"Pipeline 3 configuration written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logstash_config_pipeline_4 = \"\"\"\n",
    "input {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topics => [\"indicators-stream\"]\n",
    "    group_id => \"detect-threat-source-changes-consumer\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  if [details][indicators][source][country_name] {\n",
    "    mutate {\n",
    "      add_field => { \"source_country\" => \"%{[details][indicators][source][countryName]}\" }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Check for changes in source country/region and flag them\n",
    "  if ([source_country] != [previous_source_country]) {\n",
    "    mutate {\n",
    "      add_field => { \"source_change\" => \"true\" }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  mutate {\n",
    "    add_field => { \"previous_source_country\" => \"%{source_country}\" }\n",
    "  }\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topic_id => \"threat-source-country-change-report\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "  stdout {\n",
    "    codec => rubydebug\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "# Step 2: Write Pipeline 4 config to a file\n",
    "logstash_config_path_4 = r\"C:\\logstash-8.16.1\\pipeline_4.conf\"\n",
    "with open(logstash_config_path_4, 'w', encoding='utf-8') as f:\n",
    "    f.write(logstash_config_pipeline_4)\n",
    "print(\"Pipeline 4 configuration written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define paths for Logstash configuration files\n",
    "logstash_bin_path = \"C:/logstash-8.16.1/bin/logstash.bat\"  # Correct path to Logstash executable\n",
    "logstash_config_paths = [\n",
    "    \"C:\\logstash-8.16.1\\pipeline_1.conf\",\n",
    "    \"C:\\logstash-8.16.1\\pipeline_2.conf\",\n",
    "    \"C:\\logstash-8.16.1\\pipeline_3.conf\",\n",
    "    \"C:\\logstash-8.16.1\\pipeline_4.conf\"\n",
    "]\n",
    "\n",
    "# Run Logstash with the configuration files\n",
    "for config_path in logstash_config_paths:\n",
    "    command = [logstash_bin_path, \"-f\", config_path]\n",
    "    print(f\"Running Logstash with config: {config_path}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        \n",
    "        # Capture and print stdout and stderr\n",
    "        print(f\"Standard Output:\\n{result.stdout}\")\n",
    "        print(f\"Standard Error:\\n{result.stderr}\")\n",
    "        \n",
    "        # Check if there was an error (non-zero exit code)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Logstash command failed with exit code {result.returncode}\")\n",
    "            raise RuntimeError(f\"Logstash command failed with exit code {result.returncode}\")\n",
    "        else:\n",
    "            print(\"Logstash command executed successfully.\")\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running Logstash: {e}\")\n",
    "        raise RuntimeError(f\"command '{e.cmd}' return with error (code {e.returncode}): {e.output}\")\n"
   ]
>>>>>>> 6f794fb342af633b5d1579151186bd7a112962cb
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

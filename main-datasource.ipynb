{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kafka data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, Consumer, KafkaException, KafkaError\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Function to initialize Kafka producer\n",
    "def create_kafka_producer(bootstrap_servers):\n",
    "    return Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "# Function to send data to Kafka\n",
    "def send_to_kafka(producer, topic, key, value):\n",
    "    producer.produce(topic, key=key, value=value)\n",
    "    producer.flush()  # Make sure to flush after sending\n",
    "\n",
    "# Function to process and send indicators in batches\n",
    "def process_and_send_to_kafka(indicators, producer, topic, batch_size=100):\n",
    "    for i in range(0, len(indicators), batch_size):\n",
    "        batch = indicators[i:i + batch_size]\n",
    "        serialized_batch = json.dumps(batch)  # Serialize batch to JSON\n",
    "        print(f\"Sending batch {i // batch_size + 1} to Kafka...\")\n",
    "        send_to_kafka(producer, topic, key=None, value=serialized_batch)\n",
    "\n",
    "# Function to initialize Kafka consumer\n",
    "def create_kafka_consumer(bootstrap_servers, group_id):\n",
    "    return Consumer({\n",
    "        'bootstrap.servers': bootstrap_servers,\n",
    "        'group.id': group_id,\n",
    "        'auto.offset.reset': 'earliest'\n",
    "    })\n",
    "\n",
    "# Function to consume a limited number of messages from Kafka\n",
    "def consume_from_kafka(consumer, topic, message_count=10):\n",
    "    consumer.subscribe([topic])\n",
    "    messages = []\n",
    "    try:\n",
    "        for _ in range(message_count):\n",
    "            msg = consumer.poll(timeout=1.0)  # 1 second timeout\n",
    "            if msg is None:\n",
    "                continue  # No message, just continue\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    print(f\"End of partition reached {msg.topic()} {msg.partition()} offset {msg.offset()}\")\n",
    "                else:\n",
    "                    raise KafkaException(msg.error())\n",
    "            else:\n",
    "                messages.append(msg.value().decode('utf-8'))  # Store the message content\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        consumer.close()\n",
    "    return messages\n",
    "\n",
    "# Main execution function for running in a notebook\n",
    "def main():\n",
    "    # Kafka Configuration\n",
    "    KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'  # Confluent Kafka broker address\n",
    "    KAFKA_TOPIC = 'indicators-stream'  # Correct topic name where messages should be sent\n",
    "    KAFKA_GROUP_ID = 'test-consumer-group'  # Consumer group ID\n",
    "\n",
    "    # Create Kafka producer\n",
    "    producer = create_kafka_producer(KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "    # Example indicator data (for testing)\n",
    "    indicators = [{'id': i, 'value': f'Indicator {i}'} for i in range(500)]  # Simulate 500 indicators\n",
    "\n",
    "    # Process and send indicators to Kafka (only sending a chunk of 100 for testing in notebook)\n",
    "    CHUNK_SIZE = 100  # Number of indicators per batch\n",
    "    limited_indicators = indicators[:CHUNK_SIZE]\n",
    "    print(f\"Processing and sending the first {len(limited_indicators)} indicators to Kafka...\")\n",
    "\n",
    "    process_and_send_to_kafka(limited_indicators, producer, KAFKA_TOPIC)\n",
    "\n",
    "    # Create Kafka consumer to consume messages\n",
    "    consumer = create_kafka_consumer(KAFKA_BOOTSTRAP_SERVERS, KAFKA_GROUP_ID)\n",
    "\n",
    "    # Consume a limited number of messages from Kafka\n",
    "    print(\"Consuming messages from Kafka...\")\n",
    "    messages = consume_from_kafka(consumer, KAFKA_TOPIC, message_count=10)\n",
    "\n",
    "    # Print the consumed messages\n",
    "    for msg in messages:\n",
    "        print(f\"Consumed message: {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and sending the first 100 indicators to Kafka...\n",
      "Sending batch 1 to Kafka...\n",
      "Consuming messages from Kafka...\n",
      "Consumed message: [{\"id\": 0, \"value\": \"Indicator 0\"}, {\"id\": 1, \"value\": \"Indicator 1\"}, {\"id\": 2, \"value\": \"Indicator 2\"}, {\"id\": 3, \"value\": \"Indicator 3\"}, {\"id\": 4, \"value\": \"Indicator 4\"}, {\"id\": 5, \"value\": \"Indicator 5\"}, {\"id\": 6, \"value\": \"Indicator 6\"}, {\"id\": 7, \"value\": \"Indicator 7\"}, {\"id\": 8, \"value\": \"Indicator 8\"}, {\"id\": 9, \"value\": \"Indicator 9\"}, {\"id\": 10, \"value\": \"Indicator 10\"}, {\"id\": 11, \"value\": \"Indicator 11\"}, {\"id\": 12, \"value\": \"Indicator 12\"}, {\"id\": 13, \"value\": \"Indicator 13\"}, {\"id\": 14, \"value\": \"Indicator 14\"}, {\"id\": 15, \"value\": \"Indicator 15\"}, {\"id\": 16, \"value\": \"Indicator 16\"}, {\"id\": 17, \"value\": \"Indicator 17\"}, {\"id\": 18, \"value\": \"Indicator 18\"}, {\"id\": 19, \"value\": \"Indicator 19\"}, {\"id\": 20, \"value\": \"Indicator 20\"}, {\"id\": 21, \"value\": \"Indicator 21\"}, {\"id\": 22, \"value\": \"Indicator 22\"}, {\"id\": 23, \"value\": \"Indicator 23\"}, {\"id\": 24, \"value\": \"Indicator 24\"}, {\"id\": 25, \"value\": \"Indicator 25\"}, {\"id\": 26, \"value\": \"Indicator 26\"}, {\"id\": 27, \"value\": \"Indicator 27\"}, {\"id\": 28, \"value\": \"Indicator 28\"}, {\"id\": 29, \"value\": \"Indicator 29\"}, {\"id\": 30, \"value\": \"Indicator 30\"}, {\"id\": 31, \"value\": \"Indicator 31\"}, {\"id\": 32, \"value\": \"Indicator 32\"}, {\"id\": 33, \"value\": \"Indicator 33\"}, {\"id\": 34, \"value\": \"Indicator 34\"}, {\"id\": 35, \"value\": \"Indicator 35\"}, {\"id\": 36, \"value\": \"Indicator 36\"}, {\"id\": 37, \"value\": \"Indicator 37\"}, {\"id\": 38, \"value\": \"Indicator 38\"}, {\"id\": 39, \"value\": \"Indicator 39\"}, {\"id\": 40, \"value\": \"Indicator 40\"}, {\"id\": 41, \"value\": \"Indicator 41\"}, {\"id\": 42, \"value\": \"Indicator 42\"}, {\"id\": 43, \"value\": \"Indicator 43\"}, {\"id\": 44, \"value\": \"Indicator 44\"}, {\"id\": 45, \"value\": \"Indicator 45\"}, {\"id\": 46, \"value\": \"Indicator 46\"}, {\"id\": 47, \"value\": \"Indicator 47\"}, {\"id\": 48, \"value\": \"Indicator 48\"}, {\"id\": 49, \"value\": \"Indicator 49\"}, {\"id\": 50, \"value\": \"Indicator 50\"}, {\"id\": 51, \"value\": \"Indicator 51\"}, {\"id\": 52, \"value\": \"Indicator 52\"}, {\"id\": 53, \"value\": \"Indicator 53\"}, {\"id\": 54, \"value\": \"Indicator 54\"}, {\"id\": 55, \"value\": \"Indicator 55\"}, {\"id\": 56, \"value\": \"Indicator 56\"}, {\"id\": 57, \"value\": \"Indicator 57\"}, {\"id\": 58, \"value\": \"Indicator 58\"}, {\"id\": 59, \"value\": \"Indicator 59\"}, {\"id\": 60, \"value\": \"Indicator 60\"}, {\"id\": 61, \"value\": \"Indicator 61\"}, {\"id\": 62, \"value\": \"Indicator 62\"}, {\"id\": 63, \"value\": \"Indicator 63\"}, {\"id\": 64, \"value\": \"Indicator 64\"}, {\"id\": 65, \"value\": \"Indicator 65\"}, {\"id\": 66, \"value\": \"Indicator 66\"}, {\"id\": 67, \"value\": \"Indicator 67\"}, {\"id\": 68, \"value\": \"Indicator 68\"}, {\"id\": 69, \"value\": \"Indicator 69\"}, {\"id\": 70, \"value\": \"Indicator 70\"}, {\"id\": 71, \"value\": \"Indicator 71\"}, {\"id\": 72, \"value\": \"Indicator 72\"}, {\"id\": 73, \"value\": \"Indicator 73\"}, {\"id\": 74, \"value\": \"Indicator 74\"}, {\"id\": 75, \"value\": \"Indicator 75\"}, {\"id\": 76, \"value\": \"Indicator 76\"}, {\"id\": 77, \"value\": \"Indicator 77\"}, {\"id\": 78, \"value\": \"Indicator 78\"}, {\"id\": 79, \"value\": \"Indicator 79\"}, {\"id\": 80, \"value\": \"Indicator 80\"}, {\"id\": 81, \"value\": \"Indicator 81\"}, {\"id\": 82, \"value\": \"Indicator 82\"}, {\"id\": 83, \"value\": \"Indicator 83\"}, {\"id\": 84, \"value\": \"Indicator 84\"}, {\"id\": 85, \"value\": \"Indicator 85\"}, {\"id\": 86, \"value\": \"Indicator 86\"}, {\"id\": 87, \"value\": \"Indicator 87\"}, {\"id\": 88, \"value\": \"Indicator 88\"}, {\"id\": 89, \"value\": \"Indicator 89\"}, {\"id\": 90, \"value\": \"Indicator 90\"}, {\"id\": 91, \"value\": \"Indicator 91\"}, {\"id\": 92, \"value\": \"Indicator 92\"}, {\"id\": 93, \"value\": \"Indicator 93\"}, {\"id\": 94, \"value\": \"Indicator 94\"}, {\"id\": 95, \"value\": \"Indicator 95\"}, {\"id\": 96, \"value\": \"Indicator 96\"}, {\"id\": 97, \"value\": \"Indicator 97\"}, {\"id\": 98, \"value\": \"Indicator 98\"}, {\"id\": 99, \"value\": \"Indicator 99\"}]\n"
     ]
    }
   ],
   "source": [
    "# Run the main function in the notebook\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1 configuration written successfully.\n",
      "Pipeline 2 configuration written successfully.\n",
      "Pipeline 3 configuration written successfully.\n",
      "Pipeline 4 configuration written successfully.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the Logstash pipeline configuration for Pipeline 1\n",
    "logstash_config_pipeline_1 = \"\"\"\n",
    "input {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topics => [\"indicators-stream\"]\n",
    "    group_id => \"top-10-target-country-consumer\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  if [details][indicators][ip][country_name] {\n",
    "    mutate {\n",
    "      add_field => { \"target_country\" => \"%{[details][indicators][ip][country_name]}\" }\n",
    "    }\n",
    "    mutate {\n",
    "      add_field => { \"count\" => 1 }\n",
    "    }\n",
    "  } else {\n",
    "    drop { }\n",
    "  }\n",
    "}\n",
    "\n",
    "aggregate {\n",
    "  task_id => \"%{target_country}\"\n",
    "  code => \"\n",
    "    map['count'] ||= 0\n",
    "    map['count'] += event.get('count').to_i\n",
    "  \"\n",
    "  push_previous_map_as_event => true\n",
    "  timeout => 60\n",
    "}\n",
    "\n",
    "ruby {\n",
    "  code => \"\n",
    "    event.set('sorted_countries', event.get('aggregate_maps').values.sort_by { |entry| -entry['count'] }.first(10))\n",
    "  \"\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topic_id => \"top-target-countries\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "  stdout {\n",
    "    codec => rubydebug\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Define the Logstash pipeline configuration for Pipeline 2\n",
    "logstash_config_pipeline_2 = \"\"\"\n",
    "input {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topics => [\"indicators-stream\"]\n",
    "    group_id => \"top-10-threat-source-consumer\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  if [details][indicators][source][country_name] {\n",
    "    mutate {\n",
    "      add_field => { \"source_country\" => \"%{[details][indicators][source][country_name]}\" }\n",
    "    }\n",
    "    mutate {\n",
    "      add_field => { \"count\" => 1 }\n",
    "    }\n",
    "  } else {\n",
    "    drop { }\n",
    "  }\n",
    "}\n",
    "\n",
    "aggregate {\n",
    "  task_id => \"%{source_country}\"\n",
    "  code => \"\n",
    "    map['count'] ||= 0\n",
    "    map['count'] += event.get('count').to_i\n",
    "  \"\n",
    "  push_previous_map_as_event => true\n",
    "  timeout => 60\n",
    "}\n",
    "\n",
    "ruby {\n",
    "  code => \"\n",
    "    event.set('sorted_source_countries', event.get('aggregate_maps').values.sort_by { |entry| -entry['count'] }.first(10))\n",
    "  \"\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topic_id => \"top-threat-source-countries\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "  stdout {\n",
    "    codec => rubydebug\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Define the Logstash pipeline configuration for Pipeline 3\n",
    "logstash_config_pipeline_3 = \"\"\"\n",
    "input {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topics => [\"indicators-stream\"]\n",
    "    group_id => \"detect-target-changes-consumer\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  if [details][indicators][ip][country_name] {\n",
    "    mutate {\n",
    "      add_field => { \"target_country\" => \"%{[details][indicators][ip][country_name]}\" }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Check for changes in country/region and flag them\n",
    "  if ([target_country] != [previous_target_country]) {\n",
    "    mutate {\n",
    "      add_field => { \"country_change\" => \"true\" }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  mutate {\n",
    "    add_field => { \"previous_target_country\" => \"%{target_country}\" }\n",
    "  }\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topic_id => \"target-country-change-report\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "  stdout {\n",
    "    codec => rubydebug\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Define the Logstash pipeline configuration for Pipeline 4\n",
    "logstash_config_pipeline_4 = \"\"\"\n",
    "input {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topics => [\"indicators-stream\"]\n",
    "    group_id => \"detect-threat-source-changes-consumer\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "  if [details][indicators][source][country_name] {\n",
    "    mutate {\n",
    "      add_field => { \"source_country\" => \"%{[details][indicators][source][country_name]}\" }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Check for changes in source country/region and flag them\n",
    "  if ([source_country] != [previous_source_country]) {\n",
    "    mutate {\n",
    "      add_field => { \"source_change\" => \"true\" }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  mutate {\n",
    "    add_field => { \"previous_source_country\" => \"%{source_country}\" }\n",
    "  }\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    bootstrap_servers => \"localhost:9092\"\n",
    "    topic_id => \"threat-source-country-change-report\"\n",
    "    codec => \"json\"\n",
    "  }\n",
    "  stdout {\n",
    "    codec => rubydebug\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# File paths where we will save the configuration files\n",
    "logstash_config_path_1 = r\"C:/Users/I745988/logstash-8.16.1/bin/pipeline_1.conf\"\n",
    "\n",
    "logstash_config_path_2 = r\"C:/Users/I745988/logstash-8.16.1/bin/pipeline_2.conf\"\n",
    "logstash_config_path_3 = r\"C:/Users/I745988/logstash-8.16.1/bin/pipeline_3.conf\"\n",
    "logstash_config_path_4 = r\"C:/Users/I745988/logstash-8.16.1/bin/pipeline_4.conf\"\n",
    "\n",
    "# Write the configuration contents to the respective files\n",
    "with open(logstash_config_path_1, 'w', encoding='utf-8') as f:\n",
    "    f.write(logstash_config_pipeline_1)\n",
    "print(\"Pipeline 1 configuration written successfully.\")\n",
    "\n",
    "with open(logstash_config_path_2, 'w', encoding='utf-8') as f:\n",
    "    f.write(logstash_config_pipeline_2)\n",
    "print(\"Pipeline 2 configuration written successfully.\")\n",
    "\n",
    "with open(logstash_config_path_3, 'w', encoding='utf-8') as f:\n",
    "    f.write(logstash_config_pipeline_3)\n",
    "print(\"Pipeline 3 configuration written successfully.\")\n",
    "\n",
    "with open(logstash_config_path_4, 'w', encoding='utf-8') as f:\n",
    "    f.write(logstash_config_pipeline_4)\n",
    "print(\"Pipeline 4 configuration written successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Logstash with config: C:\\Users\\I745988\\logstash-8.16.1\\bin\\pipeline_1.conf\n",
      "Command to be executed: C:/Users/I745988/logstash-8.16.1/bin/logstash.bat -f C:\\Users\\I745988\\logstash-8.16.1\\bin\\pipeline_1.conf\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the Logstash executable path and the paths to the config files\n",
    "logstash_bin_path = \"C:/Users/I745988/logstash-8.16.1/bin/logstash.bat\"  # Ensure this points to the correct Logstash executable\n",
    "logstash_config_paths = [\n",
    "    \"C:\\\\Users\\\\I745988\\\\logstash-8.16.1\\\\bin\\\\pipeline_1.conf\",\n",
    "]\n",
    "\n",
    "# Run Logstash for each configuration file\n",
    "for config_path in logstash_config_paths:\n",
    "    # Ensure the config path is not empty and valid\n",
    "    if not config_path or not config_path.strip():\n",
    "        print(f\"Invalid configuration file path: {config_path}\")\n",
    "        continue\n",
    "\n",
    "    command = [logstash_bin_path, \"-f\", config_path, \"--config.test_and_exit\"]\n",
    "    print(f\"Running Logstash with config: {config_path}\")\n",
    "    print(f\"Command to be executed: {' '.join(command)}\")  # Log the full command\n",
    "\n",
    "    try:\n",
    "        # Run the Logstash command and capture its output\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "        # Print standard output and error\n",
    "        print(f\"Standard Output:\\n{result.stdout}\")\n",
    "        print(f\"Standard Error:\\n{result.stderr}\")\n",
    "\n",
    "        # Check if there was an error (non-zero exit code)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Logstash command failed with exit code {result.returncode}\")\n",
    "            raise RuntimeError(f\"Logstash command failed with exit code {result.returncode}\")\n",
    "        else:\n",
    "            print(\"Logstash command executed successfully.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running Logstash: {e}\")\n",
    "        raise RuntimeError(f\"command '{e.cmd}' returned with error (code {e.returncode}): {e.output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "import subprocess\n",
    "\n",
    "# Ensure JAVA_HOME is set\n",
    "os.environ['JAVA_HOME'] = \"C:\\\\Program Files\\\\SapMachine\\\\JDK\\\\17\"  # Update with your JDK path\n",
    "java_path = os.path.join(os.environ['JAVA_HOME'], 'bin', 'java.exe')\n",
    "\n",
    "# Check if Java is accessible\n",
    "if not os.path.exists(java_path):\n",
    "    raise FileNotFoundError(f\"Java not found at {java_path}. Please check your JAVA_HOME setting.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Logstash with config: C:/Users/I745988/Downloads/logstash-8.16.1-windows-x86_64/logstash-8.16.1/pipeline_1.conf\n",
      "Standard Output:\n",
      "\"Using system java: \"C:\\Program Files\\SapMachine\\JDK\\17\\bin\\java.exe\"\"\n",
      "\n",
      "Standard Error:\n",
      "The system cannot find the path specified.\n",
      "could not find java; set JAVA_HOME or ensure java is in PATH \n",
      "\n",
      "Logstash command failed with exit code 1\n",
      "Unexpected error: Logstash command failed with exit code 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the Logstash executable path and config files\n",
    "logstash_bin_path = \"C:/Program Files/logstash-8.16.1/bin/benchmark.bat\"\n",
    "logstash_config_paths = [\n",
    "    \"C:/Users/I745988/Downloads/logstash-8.16.1-windows-x86_64/logstash-8.16.1/pipeline_1.conf\",  # Update with actual paths\n",
    "]\n",
    "\n",
    "# Run Logstash for each configuration file\n",
    "for config_path in logstash_config_paths:\n",
    "    command = [logstash_bin_path, \"-f\", config_path]\n",
    "    print(f\"Running Logstash with config: {config_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Run the Logstash command\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        \n",
    "        # Print output\n",
    "        print(f\"Standard Output:\\n{result.stdout}\")\n",
    "        print(f\"Standard Error:\\n{result.stderr}\")\n",
    "        \n",
    "        # Handle non-zero exit codes\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Logstash command failed with exit code {result.returncode}\")\n",
    "            raise RuntimeError(f\"Logstash command failed with exit code {result.returncode}\")\n",
    "        else:\n",
    "            print(\"Logstash command executed successfully.\")\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running Logstash: {e}\")\n",
    "        raise RuntimeError(f\"Command '{e.cmd}' returned with error (code {e.returncode}): {e.output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
